{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arva29/AMDproject/blob/main/UkraineWarTweetsProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3U8gDVuMsaRK"
      },
      "outputs": [],
      "source": [
        "# this is meant to be run on google colab\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9NE-kyYnseaE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.5-bin-hadoop2.7\") #SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sQiCus2qkEwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "753bbcd6-b5a9-4be0-a815-76eb21422c4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dfb3657f-5460-4906-a616-69d0999fad25\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dfb3657f-5460-4906-a616-69d0999fad25\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 70 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kAPywJxlvnv",
        "outputId": "f0d0c7a3-5e24-4e86-c773-9c7c7226a46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip to /content\n",
            "100% 12.4G/12.4G [02:02<00:00, 103MB/s]\n",
            "100% 12.4G/12.4G [02:02<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sXasu1Km0rG",
        "outputId": "95d58bdd-17cc-4e60-96bf-452ea84196fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\n",
            "  inflating: 0819_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0820_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0821_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0822_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0823_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0824_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0825_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0826_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0827_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0828_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0829_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0830_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0831_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0901_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0902_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0903_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0904_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0905_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0906_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0907_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0908_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0909_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0910_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0911_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0912_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0913_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0914_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0915_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0916_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0917_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0918_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0919_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0921_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0922_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0923_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0924_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0925_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0926_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0927_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0928_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0929_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0930_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1001_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1002_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1003_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1004_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1005_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1006_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1007_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1008_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1009_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1010_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1011_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0401_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0402_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0403_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0404_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0405_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0406_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0407_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0408_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0409_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0410_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0411_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0412_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0413_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0414_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0415_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0416_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0417_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0418_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0419_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0420_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0421_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0422_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0423_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0424_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0425_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0426_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0427_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0428_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0429_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0430_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0501_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0502_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0503_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0504_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0505_to_0507_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0508_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0509_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0510_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0511_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0512_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0513_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0514_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0515_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0516_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0517_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0518_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0519_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0520_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0521_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0522_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0523_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0524_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0525_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0526_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0527_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0528_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0529_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0530_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0531_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0601_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0602_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0603_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0604_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0605_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0606_to_08_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0609_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0610_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0611_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0612_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0613_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0614_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0615_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0616_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0617_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0618_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0619_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0620_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0621_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0622_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0623_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0624_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0625_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0626_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0627_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0628_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0629_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0630_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0701_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0702_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0703_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0704_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0705_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0706_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0707_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0708_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0709_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0710_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0711_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0712_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0713_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0714_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0715_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0716_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0717_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0718_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0719_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0720_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0721_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0722_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0723_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0724_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0725_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0726_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0727_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0728_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0729_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0730_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0731_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0801_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0802_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0803_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0804_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0805_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0806_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0807_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0808_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0809_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0810_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0811_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0812_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0813_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0814_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0815_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0816_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0817_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0818_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB27.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR01.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR02.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR03.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR04.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR05.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR06.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR07.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR08.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR09.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR10.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR11.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR12.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR13.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR14.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR15.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR16.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR17.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR18.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR19.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR20.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR21.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR22.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR23.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR24.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR25.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR26.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR29.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR30.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR31.csv.gzip  \n"
          ]
        }
      ],
      "source": [
        "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXDcwYpvcX3"
      },
      "source": [
        "I extract the collection of tweets from the (23 of September ?? ) and I will work on this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p4Ir21JAqo7A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "\n",
        "data_list = []\n",
        "\n",
        "with gzip.open('1008_UkraineCombinedTweetsDeduped.csv.gzip', 'rt') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        data_list.append(row)\n",
        "\n",
        "rdd = sc.parallelize(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73CX7dxqvifD",
        "outputId": "f958da97-4a5a-46ae-a46e-8883113887e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86471"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "rdd.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU0jV4agws7o",
        "outputId": "bed30f89-c1ae-460b-ea5d-8c1e601a50f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('', '130404'),\n",
              "             ('userid', '1603709857'),\n",
              "             ('username', 'trajanpublisher'),\n",
              "             ('acctdesc',\n",
              "              \"We are Trajan Media, owner of Canada's premier numismatic and philatelic publications –\\xa0Canadian Coin News and Canadian Stamp News –\\xa0plus Coin & Stamp Supplies.\"),\n",
              "             ('location', 'Canada'),\n",
              "             ('following', '190'),\n",
              "             ('followers', '916'),\n",
              "             ('totaltweets', '7215'),\n",
              "             ('usercreatedts', '2013-07-18 15:23:57'),\n",
              "             ('tweetid', '1578535646146199554'),\n",
              "             ('tweetcreatedts', '2022-10-08 00:00:00'),\n",
              "             ('retweetcount', '1'),\n",
              "             ('text',\n",
              "              'Canada joined 15 countries whose flags grace the obverse of Ukraine’s first commemorative coin issued since the beginning of Russia’s renewed invasion this February.\\n\\nRead more: https://t.co/vRhROceH9x\\n\\n#numismatics #Ukraine️ #collectcoins'),\n",
              "             ('hashtags',\n",
              "              \"[{'text': 'numismatics', 'indices': [203, 215]}, {'text': 'Ukraine️', 'indices': [216, 225]}, {'text': 'collectcoins', 'indices': [226, 239]}]\"),\n",
              "             ('language', 'en'),\n",
              "             ('coordinates', ''),\n",
              "             ('favorite_count', '1'),\n",
              "             ('is_retweet', 'False'),\n",
              "             ('original_tweet_id', '0'),\n",
              "             ('original_tweet_userid', '0'),\n",
              "             ('original_tweet_username', ''),\n",
              "             ('in_reply_to_status_id', '0'),\n",
              "             ('in_reply_to_user_id', '0'),\n",
              "             ('in_reply_to_screen_name', ''),\n",
              "             ('is_quote_status', 'False'),\n",
              "             ('quoted_status_id', '0'),\n",
              "             ('quoted_status_userid', '0'),\n",
              "             ('quoted_status_username', ''),\n",
              "             ('extractedts', '2022-10-08 05:58:05.830392')])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZfZcFbiFVm"
      },
      "source": [
        "Tokenize tweet by removing puntuation and stop words in order to have only meaningful words to understand the meaning of the tweet. I also put all the words lower case and remove the link at the end of the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb1q3C3OxU6q",
        "outputId": "fd81d10a-18f0-487f-a07b-64024565bcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=09e840f8d893a59a272141061efbbf0b8825a695b77a7075f502b3df02c752dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKGpVdvFf1qs",
        "outputId": "050f233d-e677-48c8-ebcf-b21e38ff5e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from cleantext import clean\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopw = set(stopwords.words('english'))\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "#tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation and word not in stopw]\n",
        "tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjNQnsNLhLJo",
        "outputId": "e5f54e2a-e4f7-4c31-9fd1-9c604d36bec2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " ['canada',\n",
              "  'joined',\n",
              "  '15',\n",
              "  'countries',\n",
              "  'whose',\n",
              "  'flags',\n",
              "  'grace',\n",
              "  'the',\n",
              "  'obverse',\n",
              "  'of',\n",
              "  \"ukraine's\",\n",
              "  'first',\n",
              "  'commemorative',\n",
              "  'coin',\n",
              "  'issued',\n",
              "  'since',\n",
              "  'the',\n",
              "  'beginning',\n",
              "  'of',\n",
              "  \"russia's\",\n",
              "  'renewed',\n",
              "  'invasion',\n",
              "  'this',\n",
              "  'february',\n",
              "  'read',\n",
              "  'more',\n",
              "  '#numismatics',\n",
              "  '#ukraine',\n",
              "  '#collectcoins'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sort(list):\n",
        "  list.sort()\n",
        "  return list\n",
        "\n",
        "en_tweets = rdd.filter(lambda x: x.get(\"language\")=='en').map(lambda x: (x, x.get(\"text\")))\n",
        "\n",
        "tweets_tokenized = en_tweets.map(lambda x: (x[0].get(\"tweetid\"), tokenize(x[1])))\n",
        "\n",
        "tweets_tokenized.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GDwUytD4mg",
        "outputId": "9af16406-7bff-4528-b5ed-53833c6f9bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+\n",
            "|           tweet_id|              tokens|\n",
            "+-------------------+--------------------+\n",
            "|1578535646146199554|[canada, joined, ...|\n",
            "|1578535651984650240|[good, night, #kh...|\n",
            "|1578535652248850432|[us, invests, 290...|\n",
            "|1578535654149038080|[go, fuck, yourse...|\n",
            "|1578535658691629056|[today, 26:30, wa...|\n",
            "|1578535679713497089|[#saynomore, #usa...|\n",
            "|1578535680585916417|[new, podcast, sb...|\n",
            "|1578535686818304000|[another, deluded...|\n",
            "|1578535704694779907|[liberation, is, ...|\n",
            "|1578535714945658882|[perhaps, we, jus...|\n",
            "|1578535720276590593|[thank, the, #usa...|\n",
            "|1578535722335682560|[to, the, tune, o...|\n",
            "|1578535723413950464|[the, world, does...|\n",
            "|1578535726266060800|[the, next, 2, ye...|\n",
            "|1578535753834934272|[zelensky, the, w...|\n",
            "|1578535760596463616|[@lesiavasylenko,...|\n",
            "|1578535802534907904|[#biden, finalize...|\n",
            "|1578535866087022592|[grand, larceny, ...|\n",
            "|1578535891491954688|[#weekendvibes, #...|\n",
            "|1578535911444254720|[#odessa, #odesa,...|\n",
            "+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType\n",
        "\n",
        "dfSchema = StructType([       \n",
        "    StructField('tweet_id', StringType(), True),\n",
        "    StructField('tokens', ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "tmp = tweets_tokenized.map(lambda x: (x[0], x[1]))\n",
        "tweetsDF = spark.createDataFrame(tmp, dfSchema)\n",
        "\n",
        "tweetsDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqYhIj4w2sM"
      },
      "source": [
        "## *K*-Shingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzQVwkvSNKwr",
        "outputId": "9c13790e-668a-4208-de26-b48dbf0c0ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number K for shingles dimension: 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " ['#numismatics #ukraine #collectcoins',\n",
              "  '15 countries whose',\n",
              "  \"beginning of russia's\",\n",
              "  'canada joined 15',\n",
              "  'coin issued since',\n",
              "  'commemorative coin issued',\n",
              "  'countries whose flags',\n",
              "  'february read more',\n",
              "  'first commemorative coin',\n",
              "  'flags grace the',\n",
              "  'grace the obverse',\n",
              "  'invasion this february',\n",
              "  'issued since the',\n",
              "  'joined 15 countries',\n",
              "  'more #numismatics #ukraine',\n",
              "  \"obverse of ukraine's\",\n",
              "  \"of russia's renewed\",\n",
              "  \"of ukraine's first\",\n",
              "  'read more #numismatics',\n",
              "  'renewed invasion this',\n",
              "  \"russia's renewed invasion\",\n",
              "  'since the beginning',\n",
              "  'the beginning of',\n",
              "  'the obverse of',\n",
              "  'this february read',\n",
              "  \"ukraine's first commemorative\",\n",
              "  'whose flags grace'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "K = int(input(\"Enter the number K for shingles dimension: \"))\n",
        "\n",
        "def build_shingles(tokens, K):\n",
        "  shingles = list()\n",
        "  tmp = \"\"\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    tmp = tokens[i]\n",
        "    if((len(tokens) - (i)) < K):\n",
        "      break;\n",
        "    for k in range(K - 1):\n",
        "      tmp += \" \" + tokens[i + (k + 1)]\n",
        "    shingles.append(tmp)\n",
        "\n",
        "  return shingles\n",
        "\n",
        "shingles_rdd = tweetsDF.rdd.map(lambda x: (x[0], sort(build_shingles(x[1], K))))\n",
        "shingles_rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cast from list to set and then to list again in order to remove all the duplicate elements"
      ],
      "metadata": {
        "id": "hCHEA17OgvdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFcbUsQtcFA",
        "outputId": "c15985e0-a152-4e46-f0b6-a1e07ecd71e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['##biggerthanme #osimhen #dazn',\n",
              " '##crimea #tma2022 #thefactmusicawards2022',\n",
              " '##crimea as #ukraine',\n",
              " '##dpr allied forces',\n",
              " '##droz #democraticviolence #gpexplorer',\n",
              " '##iranianlivesmatter #iranprotests2022 #tv3newday',\n",
              " '##kimjongun and #danielortega',\n",
              " '##kpworldtoursingapore #kerch #weverse',\n",
              " '##marvel #ironman #thor',\n",
              " '##nctdream #pakistanzindabad #case143']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "shingles_bag = sort(list(set(shingles_rdd.flatMap(lambda x: x[1]).collect())))\n",
        "shingles_bag[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WwWmOI1kJ9U"
      },
      "source": [
        "I use integers from 0 to *n* (with n equal to the length of shgles_bag) to store the shingles list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqt0SGYGd6eJ",
        "outputId": "4e3d2707-7a4a-42fe-aabe-ab628c11e3de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572145"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "shingles_bag_id = list(range(0, len(shingles_bag)))\n",
        "len(shingles_bag_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convertShingles(shingles, BoS):\n",
        "  listToReturn = [0]*len(BoS)\n",
        "  start_index = 0;\n",
        "\n",
        "  for s in shingles:\n",
        "    for i in range(start_index, len(BoS)):\n",
        "      if(s == BoS[i]):\n",
        "        listToReturn[i] = 1\n",
        "        start_index = i\n",
        "        break;\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "shingles_int_rdd = shingles_rdd.map(lambda x: (x[0], convertShingles(x[1], shingles_bag)))\n",
        "shingles_int_rdd.first()"
      ],
      "metadata": {
        "id": "kGqnNjt6wpid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hashing function for permutation"
      ],
      "metadata": {
        "id": "H-zwYzop2NDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generatePermutation(arrayToHash, a, b):\n",
        "  arrayToReturn = arrayToHash.copy()\n",
        "  \n",
        "  for i in range(len(arrayToHash)):\n",
        "    arrayToReturn[i] = (a * arrayToHash[i] + b) % len(arrayToHash)\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "n = int(input(\"Insert number n of permutation desired: \"))\n",
        "permutation_matrix = list()\n",
        "#shingles_bag_id = list(range(0, 1000000))\n",
        "\n",
        "for i in range(n):\n",
        "  #Generate 2 random numbers in order to have different hashing by randomly chaging only these two numbers\n",
        "  a = np.random.randint(1000) \n",
        "  b = np.random.randint(1000)\n",
        "  #print(a, b)\n",
        "\n",
        "  permutation_matrix.append(generatePermutation(shingles_bag_id, a, b))\n",
        "  print(permutation_matrix[i][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB5-jLT-2MJA",
        "outputId": "14cd1120-3311-46b3-f7fa-be8fbabd5bed"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert number n of permutation desired: 10\n",
            "[523888, 94963, 238183, 381403, 524623, 95698, 238918, 382138, 525358, 96433, 239653, 382873, 526093, 97168, 240388, 383608, 526828, 97903, 241123, 384343, 527563, 98638, 241858, 385078, 528298, 99373, 242593, 385813, 529033, 100108]\n",
            "[129836, 494711, 287441, 80171, 445046, 237776, 30506, 395381, 188111, 552986, 345716, 138446, 503321, 296051, 88781, 453656, 246386, 39116, 403991, 196721, 561596, 354326, 147056, 511931, 304661, 97391, 462266, 254996, 47726, 412601]\n",
            "[417564, 208719, 572019, 363174, 154329, 517629, 308784, 99939, 463239, 254394, 45549, 408849, 200004, 563304, 354459, 145614, 508914, 300069, 91224, 454524, 245679, 36834, 400134, 191289, 554589, 345744, 136899, 500199, 291354, 82509]\n",
            "[247547, 155672, 63797, 544067, 452192, 360317, 268442, 176567, 84692, 564962, 473087, 381212, 289337, 197462, 105587, 13712, 493982, 402107, 310232, 218357, 126482, 34607, 514877, 423002, 331127, 239252, 147377, 55502, 535772, 443897]\n",
            "[162181, 550996, 367666, 184336, 1006, 389821, 206491, 23161, 411976, 228646, 45316, 434131, 250801, 67471, 456286, 272956, 89626, 478441, 295111, 111781, 500596, 317266, 133936, 522751, 339421, 156091, 544906, 361576, 178246, 567061]\n",
            "[128078, 6803, 457673, 336398, 215123, 93848, 544718, 423443, 302168, 180893, 59618, 510488, 389213, 267938, 146663, 25388, 476258, 354983, 233708, 112433, 563303, 442028, 320753, 199478, 78203, 529073, 407798, 286523, 165248, 43973]\n",
            "[236445, 108975, 553650, 426180, 298710, 171240, 43770, 488445, 360975, 233505, 106035, 550710, 423240, 295770, 168300, 40830, 485505, 358035, 230565, 103095, 547770, 420300, 292830, 165360, 37890, 482565, 355095, 227625, 100155, 544830]\n",
            "[8718, 402573, 224283, 45993, 439848, 261558, 83268, 477123, 298833, 120543, 514398, 336108, 157818, 551673, 373383, 195093, 16803, 410658, 232368, 54078, 447933, 269643, 91353, 485208, 306918, 128628, 522483, 344193, 165903, 559758]\n",
            "[293618, 116903, 512333, 335618, 158903, 554333, 377618, 200903, 24188, 419618, 242903, 66188, 461618, 284903, 108188, 503618, 326903, 150188, 545618, 368903, 192188, 15473, 410903, 234188, 57473, 452903, 276188, 99473, 494903, 318188]\n",
            "[283267, 398452, 513637, 56677, 171862, 287047, 402232, 517417, 60457, 175642, 290827, 406012, 521197, 64237, 179422, 294607, 409792, 524977, 68017, 183202, 298387, 413572, 528757, 71797, 186982, 302167, 417352, 532537, 75577, 190762]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateMinhashSignature(perm_mtx, shingle_array):\n",
        "  sig_column = [np.inf] * len(perm_mtx)\n",
        "\n",
        "  for i in range(len(shingle_array)):\n",
        "    if(shingle_array[i] == 1):\n",
        "      for j in range(len(sig_column)):\n",
        "        if(perm_mtx[j][i] < sig_column[j]):\n",
        "          sig_column[j] = perm_mtx[j][i]\n",
        "\n",
        "  return sig_column\n",
        "\n",
        "minhash_sig_rdd = shingles_int_rdd.map(lambda x: (x[0], calculateMinhashSignature(permutation_matrix, x[1])))\n",
        "minhash_sig_rdd.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHz1RlZcGVYz",
        "outputId": "04b456d6-e2d3-4648-f1da-9b3e094de59e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1578535646146199554',\n",
              "  [1618, 22421, 24339, 9092, 9196, 17828, 11010, 2838, 27653, 14362]),\n",
              " ('1578535651984650240',\n",
              "  [81103, 25781, 19299, 40067, 29251, 83, 4815, 75078, 10538, 18037]),\n",
              " ('1578535652248850432',\n",
              "  [8023, 112091, 22134, 117242, 105901, 4073, 97530, 108, 10748, 8062]),\n",
              " ('1578535654149038080',\n",
              "  [38998, 75551, 158529, 25682, 96241, 188663, 101625, 36543, 5918, 290617]),\n",
              " ('1578535658691629056',\n",
              "  [1933, 74081, 4389, 4472, 18016, 21608, 28545, 17223, 58208, 28012]),\n",
              " ('1578535679713497089',\n",
              "  [19993, 4151, 4704, 6992, 37966, 6593, 7440, 20793, 668, 18247]),\n",
              " ('1578535680585916417',\n",
              "  [29443, 22421, 11739, 1427, 9406, 7853, 3975, 12288, 21143, 7222]),\n",
              " ('1578535686818304000',\n",
              "  [1618, 5411, 48909, 14867, 166, 32843, 16680, 2838, 16733, 32212]),\n",
              " ('1578535704694779907',\n",
              "  [61993, 495446, 375039, 24317, 16021, 25493, 81045, 25833, 111443, 164722]),\n",
              " ('1578535714945658882',\n",
              "  [24508, 26306, 5334, 25052, 15916, 2813, 5445, 11553, 143, 12787])]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEBUG"
      ],
      "metadata": {
        "id": "xZONxbU1SCHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generatePermutation(arrayToHash, a, b):\n",
        "  arrayToReturn = arrayToHash.copy()\n",
        "  \n",
        "  for i in range(len(arrayToHash)):\n",
        "    arrayToReturn[i] = (a * arrayToHash[i] + b) % len(arrayToHash)\n",
        "\n",
        "  print(arrayToHash, a, b, arrayToReturn)\n",
        "\n",
        "  return arrayToReturn\n",
        "  \n",
        "def calculateMinhashSignature(perm_mtx, shingle_array):\n",
        "  sig_column = [np.inf] * len(perm_mtx)\n",
        "\n",
        "  for i in range(len(shingle_array)):\n",
        "    if(shingle_array[i] == 1):\n",
        "      for j in range(len(sig_column)):\n",
        "        if(perm_mtx[j][i] < sig_column[j]):\n",
        "          #print(perm_mtx[j][i])\n",
        "          sig_column[j] = perm_mtx[j][i]\n",
        "\n",
        "  return sig_column\n",
        "\n",
        "n = int(input(\"Insert number n of permutation desired: \"))\n",
        "permutation_matrix = list()\n",
        "tmp = list(range(0, 10))\n",
        "array = [0,0,0,0,1,0,0,1,1,0]\n",
        "\n",
        "print(tmp)\n",
        "\n",
        "for i in range(n):\n",
        "  #Generate 2 random numbers in order to have different hashing by randomly chaging only these two numbers\n",
        "  a = np.random.randint(10) \n",
        "  b = np.random.randint(10)\n",
        "\n",
        "  permutation_matrix.append(generatePermutation(tmp, a, b))\n",
        "  \n",
        "[print(*line) for line in permutation_matrix]\n",
        "  \n",
        "a = calculateMinhashSignature(permutation_matrix, array)\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia4-iv1RPZTn",
        "outputId": "534b4fa0-327b-4e6f-a773-34d33b2d8b7f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert number n of permutation desired: 3\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 7 8 [8, 5, 2, 9, 6, 3, 0, 7, 4, 1]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 4 5 [5, 9, 3, 7, 1, 5, 9, 3, 7, 1]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "8 5 2 9 6 3 0 7 4 1\n",
            "5 9 3 7 1 5 9 3 7 1\n",
            "0 1 2 3 4 5 6 7 8 9\n",
            "[4, 1, 4]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMylb6JD/jO8tTVXieLidMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}