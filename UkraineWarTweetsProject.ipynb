{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arva29/AMDproject/blob/main/UkraineWarTweetsProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAVZaxiWkEj-"
      },
      "source": [
        "# **AMD PROJECT**\n",
        "\n",
        "## *Project 1 - Finding similar items*\n",
        "\n",
        "\n",
        "> *The task is to implement from scratch a detector of pairs of similar tweets, considering the text column of the dataset and selecting tweets written in a given language.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcbUYRjglQwP"
      },
      "source": [
        "## **Environment preparation**\n",
        "\n",
        "These lines of code are meant to be run every time in order to properly set the work environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3U8gDVuMsaRK"
      },
      "outputs": [],
      "source": [
        "# this is meant to be run on google colab\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9NE-kyYnseaE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.3.1-bin-hadoop3\") #SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o7uaqHNmII0"
      },
      "source": [
        "## **Getting dataset**\n",
        "\n",
        "I'm going to download directly the dataset with the kaggle API. The dataset is a collection of tweets about Ukraine war divided into csv files, one per day.\n",
        "\n",
        "The dataset could be found at the following link:\n",
        "[Ukraine War Tweets dataset](https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows?resource=download) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "sQiCus2qkEwL",
        "outputId": "279b0807-f373-4010-8af1-2cbdebeae17e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-24f1ada3-ef5e-4198-94a1-25f7893ef08b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-24f1ada3-ef5e-4198-94a1-25f7893ef08b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 70 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Upload the file kaggle.json from your disk\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kAPywJxlvnv",
        "outputId": "cad1f7b2-9a43-40d1-a063-8f83241c0026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip to /content\n",
            "100% 13.2G/13.2G [01:30<00:00, 231MB/s]\n",
            "100% 13.2G/13.2G [01:30<00:00, 157MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sXasu1Km0rG"
      },
      "outputs": [],
      "source": [
        "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuZaJ2EHxPXS"
      },
      "source": [
        "## **Datasets extraction**\n",
        "\n",
        "Extracting the data from a chosen dataset and transform them in an rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXDcwYpvcX3"
      },
      "source": [
        "### *Single day dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Ir21JAqo7A",
        "outputId": "013493b6-6a0d-47df-9705-6b178ac4de62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import csv\n",
        "import gzip\n",
        "\n",
        "data_list = []\n",
        "\n",
        "with gzip.open('1008_UkraineCombinedTweetsDeduped.csv.gzip', 'rt') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        data_list.append(row)\n",
        "\n",
        "rdd = sc.parallelize(data_list)\n",
        "rdd.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_sdpJEFZJP"
      },
      "source": [
        "### *Single day dataset (5000 rows)* \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JvRyWt0FYAY",
        "outputId": "6b0b22e4-e1e6-4b65-9854-30ee15883c4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "import gzip\n",
        "\n",
        "data_list = []\n",
        "count = 0\n",
        "\n",
        "with gzip.open('1008_UkraineCombinedTweetsDeduped.csv.gzip', 'rt') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        count += 1\n",
        "        data_list.append(row)\n",
        "        if(count >= 10000): break\n",
        "\n",
        "rdd = sc.parallelize(data_list)\n",
        "rdd.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ma0d1LyaSq"
      },
      "source": [
        "### Test rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73CX7dxqvifD",
        "outputId": "60220482-8f88-49fd-84b2-7441989c8a29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86471"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU0jV4agws7o",
        "outputId": "18c25bf3-debf-49e9-a88c-75ea81461f4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('', '130404'),\n",
              "             ('userid', '1603709857'),\n",
              "             ('username', 'trajanpublisher'),\n",
              "             ('acctdesc',\n",
              "              \"We are Trajan Media, owner of Canada's premier numismatic and philatelic publications –\\xa0Canadian Coin News and Canadian Stamp News –\\xa0plus Coin & Stamp Supplies.\"),\n",
              "             ('location', 'Canada'),\n",
              "             ('following', '190'),\n",
              "             ('followers', '916'),\n",
              "             ('totaltweets', '7215'),\n",
              "             ('usercreatedts', '2013-07-18 15:23:57'),\n",
              "             ('tweetid', '1578535646146199554'),\n",
              "             ('tweetcreatedts', '2022-10-08 00:00:00'),\n",
              "             ('retweetcount', '1'),\n",
              "             ('text',\n",
              "              'Canada joined 15 countries whose flags grace the obverse of Ukraine’s first commemorative coin issued since the beginning of Russia’s renewed invasion this February.\\n\\nRead more: https://t.co/vRhROceH9x\\n\\n#numismatics #Ukraine️ #collectcoins'),\n",
              "             ('hashtags',\n",
              "              \"[{'text': 'numismatics', 'indices': [203, 215]}, {'text': 'Ukraine️', 'indices': [216, 225]}, {'text': 'collectcoins', 'indices': [226, 239]}]\"),\n",
              "             ('language', 'en'),\n",
              "             ('coordinates', ''),\n",
              "             ('favorite_count', '1'),\n",
              "             ('is_retweet', 'False'),\n",
              "             ('original_tweet_id', '0'),\n",
              "             ('original_tweet_userid', '0'),\n",
              "             ('original_tweet_username', ''),\n",
              "             ('in_reply_to_status_id', '0'),\n",
              "             ('in_reply_to_user_id', '0'),\n",
              "             ('in_reply_to_screen_name', ''),\n",
              "             ('is_quote_status', 'False'),\n",
              "             ('quoted_status_id', '0'),\n",
              "             ('quoted_status_userid', '0'),\n",
              "             ('quoted_status_username', ''),\n",
              "             ('extractedts', '2022-10-08 05:58:05.830392')])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZfZcFbiFVm"
      },
      "source": [
        "Tokenize tweet by removing puntuation and stop words in order to have only meaningful words to understand the meaning of the tweet. I also put all the words lower case and remove the link at the end of the tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh14uGbgxa2H"
      },
      "source": [
        "## **Tokenization**\n",
        "\n",
        "In order to have tweets ready to work on, I need to do the following steps:\n",
        "\n",
        "\n",
        "1.   *Filter* only english tweets\n",
        "2.   *Clear* tweets by removing emojis, tags, urls and punctuation (keeping hashtags because they give important information about the context and are useful in finding similarity)\n",
        "3.   *Tokenized* cleared tweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVzagMPCpdJX"
      },
      "source": [
        "Clean-text is a library useful to remove from string undesired substrings like email, tags, urls, line-breaks, etc.. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb1q3C3OxU6q",
        "outputId": "2632e906-4806-42df-8ba4-a7094dc7d5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=081f555a31ce34010279767992f03f96c39581337c10dfecf33e99d82ca753f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/8c/80/c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Khhze5p1wB"
      },
      "source": [
        "Two versions of the tokenize function:\n",
        "\n",
        "\n",
        "1.   Without stopwords\n",
        "2.   With stopwords\n",
        "\n",
        "I decided to work with the one with stopwords because, as stated in the textbook: \n",
        "\n",
        "> \" *for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles* \"\n",
        "\n",
        "and I thought the case of finding similar tweets was quite similar to the one of finding similar news articles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKGpVdvFf1qs",
        "outputId": "ec15a200-312e-4454-b095-832906a1165e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from cleantext import clean\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopw = set(stopwords.words('english'))\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "# Without stopwords\n",
        "#tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation and word not in stopw]\n",
        "\n",
        "# With stopwords\n",
        "tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYk-IgO3uXvp"
      },
      "source": [
        "After filtering, clearing and tokenizing I maintain the RDD of the form: \n",
        "\n",
        "> *(tweet_id, tokenized_array)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjNQnsNLhLJo",
        "outputId": "623f2e68-8ba8-488c-d4b0-08e3f13594c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " ['canada',\n",
              "  'joined',\n",
              "  '15',\n",
              "  'countries',\n",
              "  'whose',\n",
              "  'flags',\n",
              "  'grace',\n",
              "  'the',\n",
              "  'obverse',\n",
              "  'of',\n",
              "  \"ukraine's\",\n",
              "  'first',\n",
              "  'commemorative',\n",
              "  'coin',\n",
              "  'issued',\n",
              "  'since',\n",
              "  'the',\n",
              "  'beginning',\n",
              "  'of',\n",
              "  \"russia's\",\n",
              "  'renewed',\n",
              "  'invasion',\n",
              "  'this',\n",
              "  'february',\n",
              "  'read',\n",
              "  'more',\n",
              "  '#numismatics',\n",
              "  '#ukraine',\n",
              "  '#collectcoins'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sort(list):\n",
        "  list.sort()\n",
        "  return list\n",
        "\n",
        "en_tweets = rdd.filter(lambda x: x.get(\"language\")=='en').map(lambda x: (x, x.get(\"text\")))\n",
        "\n",
        "tweets_tokenized = en_tweets.map(lambda x: (x[0].get(\"tweetid\"), tokenize(x[1])))\n",
        "\n",
        "tweets_tokenized.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqYhIj4w2sM"
      },
      "source": [
        "## ***K*-Shingles from words**\n",
        "\n",
        "From the tokenized tweet I built the list of shingle of the user desired dimension. I also sorted the list of shingles in order to speed up the next processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzQVwkvSNKwr",
        "outputId": "0150fb9c-25b6-4303-ef9f-12b7e0001da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number K for k-grams dimension: 3\n"
          ]
        }
      ],
      "source": [
        "K = int(input(\"Enter the number K for k-grams dimension: \"))\n",
        "\n",
        "def build_shingles(tokens, K):\n",
        "  shingles = list()\n",
        "  tmp = \"\"\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    tmp = tokens[i]\n",
        "    if((len(tokens) - (i)) < K):\n",
        "      break;\n",
        "    for k in range(K - 1):\n",
        "      tmp += \" \" + tokens[i + (k + 1)]\n",
        "    shingles.append(tmp)\n",
        "\n",
        "  return shingles\n",
        "\n",
        "shingles_rdd = tweets_tokenized.map(lambda x: (x[0], sort(build_shingles(x[1], K))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCHEA17OgvdG"
      },
      "source": [
        "## **Building Bag of Shingles**\n",
        "\n",
        "I build an ordered bag of shingles from the shingles of all the tweets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKduuB5m0qhY",
        "outputId": "74c1e1df-2895-40c9-b842-3c9a0a724fd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['##biggerthanme #osimhen #dazn',\n",
              " '##crimea #tma2022 #thefactmusicawards2022',\n",
              " '##crimea as #ukraine',\n",
              " '##dpr allied forces',\n",
              " '##droz #democraticviolence #gpexplorer',\n",
              " '##iranianlivesmatter #iranprotests2022 #tv3newday',\n",
              " '##kimjongun and #danielortega',\n",
              " '##kpworldtoursingapore #kerch #weverse',\n",
              " '##marvel #ironman #thor',\n",
              " '##nctdream #pakistanzindabad #case143',\n",
              " '##osce #un #ukrainerussiawar',\n",
              " '##pcr #communist regime',\n",
              " '##putin our beloved',\n",
              " \"##putin's #dictatorship #collapse\",\n",
              " '##raska appointed the',\n",
              " '##russia #ukraine is',\n",
              " '##russia and #putin',\n",
              " '##seonghwa #seonghwa_closet #tma2022',\n",
              " '##transform the #battle',\n",
              " '##trump wanted the']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "shingles_bag = sort(shingles_rdd.flatMap(lambda x: x[1]).distinct().collect())\n",
        "shingles_bag[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WwWmOI1kJ9U"
      },
      "source": [
        "I use integers from 0 to *n* (with n equal to the length of shingles_bag) to store the shingles list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqt0SGYGd6eJ",
        "outputId": "05de3aa7-2262-4f65-cc28-959a0a9a251e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572145"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "shingles_bag_id = list(range(0, len(shingles_bag)))\n",
        "len(shingles_bag_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXp3xoWYTxUg"
      },
      "source": [
        "### **Converting shingles**\n",
        "\n",
        "For each tweet I convert its shingles into integer corresponding to the position of the shingle into the Bag of Shingle array. In this way I have all the shingles encoded as integers and not as strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZvmkF4udszb",
        "outputId": "273341aa-a04b-41c2-f287-4c0aafd59073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " [33966,\n",
              "  73051,\n",
              "  154698,\n",
              "  177094,\n",
              "  185133,\n",
              "  186882,\n",
              "  192327,\n",
              "  228680,\n",
              "  232078,\n",
              "  232769,\n",
              "  253842,\n",
              "  292272,\n",
              "  300909,\n",
              "  307414,\n",
              "  338727,\n",
              "  358963,\n",
              "  366613,\n",
              "  369433,\n",
              "  407541,\n",
              "  411981,\n",
              "  420906,\n",
              "  437407,\n",
              "  469760,\n",
              "  478355,\n",
              "  493942,\n",
              "  522597,\n",
              "  552387])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def convertShingles(shingles, BoS):\n",
        "  listToReturn = list()\n",
        "  start_index = 0;\n",
        "\n",
        "  for s in shingles:\n",
        "    for i in range(start_index, len(BoS)):\n",
        "      if(s == BoS[i]):\n",
        "        listToReturn.append(i)\n",
        "        start_index = i\n",
        "        break;\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "shingles_int_rdd = shingles_rdd.map(lambda x: (x[0], convertShingles(x[1], shingles_bag)))\n",
        "#shingles_int_rdd.collect()\n",
        "shingles_int_rdd.cache()\n",
        "shingles_int_rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-zwYzop2NDp"
      },
      "source": [
        "## **Permutation matrix**\n",
        "\n",
        "Now that I have the shingles encoded as integers I can take the Bag of Shingles and generate *n* permutations of it.\n",
        "\n",
        "The permutations are generated by applying the following equation to every element of the array: \n",
        "\n",
        "\\begin{equation}\n",
        "element_{permuted} = (a \\cdot element_{before\\_permutation} + b) \\mod length_{bag\\_of\\_shingles}\n",
        "\\end{equation}\n",
        "\n",
        "Where *a* and *b* are two integers randomly generated between 0 and 1000. In this way I can generate *n* permutation only by chamging the values of *a* and *b*. This equation also guarantees that the integer resulting from it is between 0 and the length of the array, so the final array is actually a permutation of the initial one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB5-jLT-2MJA",
        "outputId": "d03b2f40-47d4-40f6-f0c1-3a9f4fe8575f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insert number n of permutation desired: 120\n"
          ]
        }
      ],
      "source": [
        "def generatePermutation(arrayToHash, a, b):\n",
        "  arrayToReturn = arrayToHash.copy()\n",
        "  \n",
        "  for i in range(len(arrayToHash)):\n",
        "    arrayToReturn[i] = (a * arrayToHash[i] + b) % len(arrayToHash)\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "n = int(input(\"Insert number n of permutation desired: \"))\n",
        "permutation_matrix = list()\n",
        "\n",
        "for i in range(n):\n",
        "  #Generate 2 random numbers in order to have different hashing by randomly changing only these two numbers\n",
        "  a = np.random.randint(len(shingles_bag_id)) \n",
        "  b = np.random.randint(len(shingles_bag_id))\n",
        "  #print(a, b)\n",
        "\n",
        "  permutation_matrix.append(generatePermutation(shingles_bag_id, a, b))\n",
        "  #print(permutation_matrix[i][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowSJK3rUTQG"
      },
      "source": [
        "## **Minhashing**\n",
        "\n",
        "For each tweet I compute the minhash signature based on the *n* permutations generated before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1nYwEj4eEBk",
        "outputId": "22263eeb-a0b5-4973-c245-30e9a54c742c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[9] at collect at <ipython-input-11-e87837df734b>:12"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def calculateMinhashSignature(perm_mtx, shingle_array):\n",
        "  sig_column = [np.inf] * len(perm_mtx)\n",
        "\n",
        "  for i in shingle_array:\n",
        "    for j in range(len(sig_column)):\n",
        "      if(perm_mtx[j][i] < sig_column[j]):\n",
        "        sig_column[j] = perm_mtx[j][i]\n",
        "\n",
        "  return sig_column\n",
        "\n",
        "minhash_sig_rdd = shingles_int_rdd.map(lambda x: (x[0], calculateMinhashSignature(permutation_matrix, x[1])))\n",
        "minhash_sig_rdd.collect()\n",
        "minhash_sig_rdd.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vvQbt2W8GGR"
      },
      "source": [
        "## **LSH**\n",
        "\n",
        "I divide the *n* minhash signatures into *b* bands of *r* rows each. I also calculate the threshold *t* based on the values of *b* and *r*.\n",
        "\n",
        "Every row will be a tuple composed as following:\n",
        "\n",
        "\n",
        "> *(tweet_information, band_information)*\n",
        "\n",
        "Where *tweet_information* are composed by *tweet_id* and the list of minhash signatures that represent the tweet, whereas *band_information* are composed by the *band_id* and the list of minhash signatures in that band.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlFPde9vRF12"
      },
      "outputs": [],
      "source": [
        "b = int(input(\"Enter into how many bands you want to divide the \" + str(n) +\" permutations: \"))\n",
        "r = int(n / b)\n",
        "\n",
        "#THRESHOLD\n",
        "t = (1/b) ** (1/r)\n",
        "print(t)\n",
        "\n",
        "def band_splitting(tweet_id, arrayToSplit, b, n):\n",
        "  tmp = list()\n",
        "  arrayToReturn = list()\n",
        "  band_id = 1\n",
        "\n",
        "  r = int(n/b)\n",
        "  i = 0\n",
        "\n",
        "  while i < len(arrayToSplit):\n",
        "    for j in range(r):\n",
        "      tmp.append(arrayToSplit[i + j])\n",
        "    \n",
        "    i += r\n",
        "    arrayToReturn.append(((tweet_id, arrayToSplit), (band_id, tmp.copy())))\n",
        "    band_id += 1\n",
        "    tmp.clear()\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "sig_bands_rdd = minhash_sig_rdd.flatMap(lambda x: band_splitting(x[0], x[1], b, n))\n",
        "#sig_bands_rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7-21sUy92cp"
      },
      "source": [
        "### **Hashing bands**\n",
        "\n",
        "I hash every bands in order to get a single integer from a list of *r* integers. To do this, I transform the list of integers into a string composed by the integers separated by a blank space (e.g. [ 1, 3, 45 ] ---> \"1 3 45\").\n",
        "\n",
        "From that, I can hash the string to an integer value. In this way, string that are equal will hash to the same number.\n",
        "\n",
        "Having bands encoded as integers allows me to get the rdd in the form of:\n",
        "\n",
        "> ( ( *hash_value*, *band_id* ), (*tweet_id*, *signatures_array*) )\n",
        "\n",
        "From that I can perform a *groupByKey* to get the list of tweets that hash to the same bucket in the same band."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouUR4pGXak6R"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "def hashArray(array):\n",
        "  col_str = ' '.join(str(x) for x in array)\n",
        "\n",
        "  return hash(col_str)\n",
        "\n",
        "sig_grouped_rdd = sig_bands_rdd.map(lambda x: ((hashArray(x[1][1]), x[1][0]), x[0])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
        "sig_grouped_rdd.collect()\n",
        "sig_grouped_rdd.persist()\n",
        "#sig_grouped_rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTllhc_9BObL"
      },
      "source": [
        "## **Group LSH results**\n",
        "\n",
        "To get for each tweet the list of others that hash to the same bucket, I do the following:\n",
        "\n",
        "\n",
        "1.   *Filter* to get only hash value for which there are more than one tweet\n",
        "2.   Apply a *flatMap* to have a rdd with tuples composed by (tweet_id, [ list_of_similar_tweets ])\n",
        "3. Apply a *reduceByKey* to remove any duplicates generated by the previous step and to join list of similar tweets corresponding to the same key tweet.\n",
        "\n",
        "After these 3 steps, I will have an rdd with only tweets that have similar tweets associated and for each of them as key I will have, as value, the list of tweet that are similar to the key tweet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tIG2cmy_sMLR"
      },
      "outputs": [],
      "source": [
        "# For each list of tweets, return a list of tuples with keys the tweets of the list \n",
        "# and as values the list of tweets given as parameter\n",
        "def array_splitting(id_array):\n",
        "  listToReturn = list()\n",
        "  \n",
        "\n",
        "  for id in id_array:\n",
        "    listToReturn.append((id[0], id_array))\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "def removeDuplicate(x, y):\n",
        "  listToReturn = x.copy()\n",
        "  x_id = [i[0] for i in x]\n",
        "  y_id = [i[0] for i in y]\n",
        "  \n",
        "  for i, id in enumerate(y_id):\n",
        "    if id not in x_id:\n",
        "      listToReturn.append(y[i]) \n",
        "\n",
        "  return listToReturn\n",
        "  \n",
        "\n",
        "similar_tweets_rdd = sig_grouped_rdd.filter(lambda x: len(x[1]) > 1).flatMap(lambda x: array_splitting(x[1])).reduceByKey(lambda x,y: removeDuplicate(x , y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrrIarbf9R8l"
      },
      "outputs": [],
      "source": [
        "similar_tweets_rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVwzC-vfBHK2"
      },
      "source": [
        "## **Calculate Jaccard Similarity**\n",
        "\n",
        "Thanks to the LSH method, I have hashed tweets to different buckets. Now I consider tweets in the same buckets as *candidate pairs* to be similar tweet. So, I can now calculate the Jaccard similarity for each of the pair of tweets that are considered *candidate pairs*.\n",
        "From this, I filter the rdd maintaing only the pairs that have a Jaccard similarity greater or equal than the threshod calculated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xcwdhP8wsuKN"
      },
      "outputs": [],
      "source": [
        "# Reconstruct as key the tuple (tweet_id, list_of_signatures)\n",
        "def getSignature(id, tweet_list):\n",
        "  sig = list()\n",
        "  for t in tweet_list:\n",
        "    if(t[0] == id):\n",
        "      sig = t[1].copy()\n",
        "      break\n",
        "  return sig\n",
        "\n",
        "# From (tweet, list_of_tweets) return a list of couples (tweet, tweet_in_the_list)\n",
        "def flatFunct(tweet, tweet_list):\n",
        "  listToReturn = list()\n",
        "  print(tweet_list)\n",
        "  for t in tweet_list:\n",
        "    print(t)\n",
        "    listToReturn.append((tweet, t))\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "# Calculate the Jaccard Similarities between the two sets given as parameters\n",
        "def calculateJaccardSimilarity(tweet_1, tweet_2):\n",
        "  d = len(tweet_1)\n",
        "  sim_count = 0\n",
        "\n",
        "  for i in range(d):\n",
        "    if (tweet_1[i] == tweet_2[i]):\n",
        "      sim_count += 1\n",
        "\n",
        "  return sim_count/d\n",
        "\n",
        "jacc_sim_rdd = similar_tweets_rdd.map(lambda x: ((x[0], getSignature(x[0], x[1])), x[1])).flatMap(lambda x: flatFunct(x[0], x[1])).map(lambda x: ((x[0][0], x[1][0]), calculateJaccardSimilarity(x[0][1], x[1][1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8MR6o1CsN9"
      },
      "source": [
        "Simply removing for each tweet itself from the list of similar tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bT9ycVS2si6",
        "outputId": "93471736-96d9-4ae2-9bda-b3317a031980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578551010993528832',\n",
              " ['1578535911444254720',\n",
              "  '1578611408467542018',\n",
              "  '1578626508062687232',\n",
              "  '1578641605610848257'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def removeId(id, array):\n",
        "  array.remove(id)\n",
        "  return array\n",
        "\n",
        "truly_similar_tweets_rdd = jacc_sim_rdd.filter(lambda x: x[1] > t).map(lambda x: x[0]).groupByKey().map(lambda x: (x[0], sort(removeId(x[0], list(x[1]))))).filter(lambda x: len(x[1]) > 1)\n",
        "truly_similar_tweets = truly_similar_tweets_rdd.collect()\n",
        "truly_similar_tweets[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ydwL_kfEruN"
      },
      "source": [
        "## **Print results**\n",
        "\n",
        "Print for each tweet the text of the similar tweets associated to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arZgvYClxNPs"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "\n",
        "for tweet, listSimilarTweets in truly_similar_tweets:\n",
        "  count += 1\n",
        "  print(\"--- TWEET ID: \" + tweet + \" ---\")\n",
        "  listSimilarTweets.insert(0, tweet)\n",
        "  [print(\" -  \" + i) for i in (rdd.filter(lambda x: x.get(\"tweetid\") in listSimilarTweets).map(lambda x: x.get(\"text\").replace('\\n', '')).collect())]\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if count == 10:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x646fz9FiZ5"
      },
      "source": [
        "# **Comparing results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clds2iV4GOQP"
      },
      "source": [
        "## **LSH from datasketch**\n",
        "\n",
        "Implementing the same procedure as above by using the [datasketch package](https://pypi.org/project/datasketch/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrT0ssjVFh2S",
        "outputId": "add2b496-0f53-421d-d8a9-a614f394503c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.8/dist-packages (1.5.8)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from datasketch) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasketch) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasketch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WTwrqvoCIHW2"
      },
      "outputs": [],
      "source": [
        "tweets_shingled = shingles_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Fb9SBwtuIWoq"
      },
      "outputs": [],
      "source": [
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "def minhashFunction(singles_array, m):\n",
        "  \n",
        "  for s in singles_array:\n",
        "    m.update(s.encode('utf8'))\n",
        "\n",
        "  return m\n",
        "\n",
        "minhash_rdd = shingles_rdd.map(lambda x: (x[0], x[1], MinHash(num_perm=n))).map(lambda x: (x[0], minhashFunction(x[1], x[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBmafhJ-ZUak",
        "outputId": "f84ee4d9-aa37-43b5-d2e6-c2350509cc53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535651984650240', <datasketch.minhash.MinHash at 0x7f1f2487e700>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "minhash_rdd.collect()[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the minhash LSH from the datasketch package to retrieve similar tweets. Then I reshape the rdd to have them as the ones from the custom algorithm."
      ],
      "metadata": {
        "id": "ohYDaRVPMwt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "DT_Z47FkK0RJ"
      },
      "outputs": [],
      "source": [
        "lsh = MinHashLSH(threshold=t, num_perm=n, params=(b,r))\n",
        "\n",
        "for e in minhash_rdd.collect():\n",
        "  lsh.insert(str(e[0]), e[1])\n",
        "\n",
        "\n",
        "datasketch_result = minhash_rdd.map(lambda x: (x[0], lsh.query(x[1]))).filter(lambda x: len(x[1])>1).map(lambda x: (x[0], sort(removeId(x[0], x[1])))).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show comparison\n",
        "\n",
        "Putting together results from the two algorithms and see where there are differences and where not "
      ],
      "metadata": {
        "id": "1Nenz9KzMmZ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PorB-DosoV",
        "outputId": "5cf831c8-f350-4117-92b0-3bdaf787aa1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '1578554508342272000',\n",
              " 'custom': ['1578535891491954688'],\n",
              " 'datasketch': ['1578535891491954688'],\n",
              " 'custom_N': 1,\n",
              " 'datasketch_N': 1,\n",
              " 'equality': True}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "def reshapeList(listToReshape):\n",
        "  listToReturn = list()\n",
        "\n",
        "  for e in listToReshape:\n",
        "    listToReturn.append(e[0])\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "comparison = list()\n",
        "similar_tweets = similar_tweets_rdd.map(lambda x: (x[0], sort(removeId(x[0], reshapeList(x[1]))))).collect()\n",
        "check = 0\n",
        "\n",
        "for e in similar_tweets:\n",
        "  for s in datasketch_result:\n",
        "    if e[0] == s[0]:\n",
        "      comparison.append({'id':e[0], 'custom':e[1], 'datasketch':s[1], 'custom_N': len(e[1]), 'datasketch_N': len(s[1]), 'equality': e[1] == s[1]})\n",
        "      check += 1\n",
        "      break\n",
        "  if check == 0:\n",
        "    comparison.append({'id':e[0], 'custom':e[1], 'datasketch':[], 'custom_N': len(e[1]), 'datasketch_N': 0, 'equality': e[1] == s[1]})\n",
        "  else:\n",
        "    check = 0\n",
        "\n",
        "comparison[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01SmYruBvx0r",
        "outputId": "75c26de6-e726-493a-9d8b-4ba074f27ea7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4983, 4998)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "len(datasketch_result), len(similar_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnWWDpUosNoo",
        "outputId": "90657717-86da-4651-8a53-4e411fc983b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4998 3701 1297\n"
          ]
        }
      ],
      "source": [
        "comparison_rdd = sc.parallelize(comparison)\n",
        "false_comparison_rdd = comparison_rdd.filter(lambda x: x.get(\"equality\") == False)\n",
        "\n",
        "totalRows = comparison_rdd.count()\n",
        "falseRows = false_comparison_rdd.count()\n",
        "trueRows = totalRows - falseRows\n",
        "\n",
        "print(totalRows,   trueRows, falseRows)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply debug print to show actual text of the desired tweet"
      ],
      "metadata": {
        "id": "_oM4C04SNhCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjN0DjQywp4L",
        "outputId": "f47a4161-67b0-40ac-9ccf-2e5ec71755bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TWEET ID: 1578536416618811396 ---\n",
            "--- CUSTOM:\n",
            " - Polish T-62 in a Lake Amuses #Russian Soldiers - 2022 Today Kherson Dir.#Russia #UkraineWar #CombatFootage #Kharkiv #Donbass #DNR #LPR #Army #RussianArmy #counteroffensive #Ukraine #Video #NATO #Ukraine  #Russians #military #kherson #combat #army https://t.co/0c1VwAnhRx\n",
            " - 'Another American Howitzer Destroyed in Donbass' - 2022 #Drone Video#Russia #UkraineWar #CombatFootage #Kharkiv #Donbass #DNR #LPR #Army #RussianArmy #counteroffensive #Ukraine #Video #NATO #Ukraine  #Russians #military #kherson #combat #army https://t.co/gTqOMQuaxz\n",
            "--- DATASKETCH:\n",
            " - 'Another American Howitzer Destroyed in Donbass' - 2022 #Drone Video#Russia #UkraineWar #CombatFootage #Kharkiv #Donbass #DNR #LPR #Army #RussianArmy #counteroffensive #Ukraine #Video #NATO #Ukraine  #Russians #military #kherson #combat #army https://t.co/gTqOMQuaxz\n",
            " - #DPR Forces Clearing #AFU Trenches South of #Marinka - 2022 Donbass Today#Russia #UkraineWar #CombatFootage #Kharkiv #Donbass #DNR #LPR #Army #RussianArmy #counteroffensive #Ukraine #Video #NATO #Ukraine  #Russians #military #kherson #combat #army https://t.co/hHuq7QxSSt\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print similar tweets for the desired id\n",
        "# '1578551010993528832'\n",
        "\n",
        "id = '1578536416618811396'\n",
        "tweetCustomToPrint = list()\n",
        "tweetDatasketchToPrint = list()\n",
        "\n",
        "for e in similar_tweets:\n",
        "  if e[0] == id:\n",
        "    tweetCustomToPrint = e[1]\n",
        "    break\n",
        "\n",
        "for e in datasketch_result:\n",
        "  if e[0] == id:\n",
        "    tweetDatasketchToPrint = e[1]\n",
        "    break\n",
        "\n",
        "print(\"--- TWEET ID: \" + str(id) + \" ---\")\n",
        "print(\"--- CUSTOM:\")\n",
        "[print(\" - \" + i) for i in rdd.filter(lambda x: x.get(\"tweetid\") in tweetCustomToPrint).map(lambda x: x.get(\"text\").replace('\\n', '')).collect()]\n",
        "print(\"--- DATASKETCH:\")\n",
        "[print(\" - \" + i) for i in rdd.filter(lambda x: x.get(\"tweetid\") in tweetDatasketchToPrint).map(lambda x: x.get(\"text\").replace('\\n', '')).collect()]\n",
        "print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV2FNmX0zMu/G22joUdRiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}