{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arva29/AMDproject/blob/main/UkraineWarTweetsProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U8gDVuMsaRK"
      },
      "outputs": [],
      "source": [
        "# this is meant to be run on google colab\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NE-kyYnseaE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.5-bin-hadoop2.7\") #SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQiCus2qkEwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "0459b414-a470-4779-81c1-28d15a333ed4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f91c2575-0c85-446d-9dac-49a64f8c04f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f91c2575-0c85-446d-9dac-49a64f8c04f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 70 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kAPywJxlvnv",
        "outputId": "89eb676f-30e9-4d44-84de-a05b316a1c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip to /content\n",
            "100% 12.5G/12.5G [02:08<00:00, 111MB/s]\n",
            "100% 12.5G/12.5G [02:08<00:00, 104MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sXasu1Km0rG",
        "outputId": "c61fa106-9f04-4fef-ea86-f0166d7340fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\n",
            "  inflating: 0819_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0820_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0821_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0822_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0823_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0824_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0825_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0826_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0827_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0828_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0829_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0830_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0831_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0901_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0902_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0903_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0904_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0905_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0906_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0907_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0908_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0909_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0910_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0911_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0912_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0913_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0914_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0915_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0916_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0917_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0918_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0919_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0921_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0922_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0923_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0924_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0925_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0926_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0927_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0928_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0929_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0930_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1001_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1002_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1003_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1004_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1005_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1006_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1007_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1008_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1009_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1010_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1011_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1012_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1013_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1014_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1015_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1016_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1017_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 1018_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0401_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0402_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0403_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0404_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0405_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0406_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0407_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0408_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0409_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0410_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0411_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0412_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0413_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0414_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0415_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0416_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0417_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0418_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0419_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0420_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0421_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0422_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0423_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0424_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0425_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0426_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0427_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0428_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0429_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0430_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0501_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0502_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0503_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0504_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0505_to_0507_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0508_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0509_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0510_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0511_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0512_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0513_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0514_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0515_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0516_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0517_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0518_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0519_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0520_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0521_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0522_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0523_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0524_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0525_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0526_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0527_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0528_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0529_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0530_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0531_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0601_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0602_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0603_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0604_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0605_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0606_to_08_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0609_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0610_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0611_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0612_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0613_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0614_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0615_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0616_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0617_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0618_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0619_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0620_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0621_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0622_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0623_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0624_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0625_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0626_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0627_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0628_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0629_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0630_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0701_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0702_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0703_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0704_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0705_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0706_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0707_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0708_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0709_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0710_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0711_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0712_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0713_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0714_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0715_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0716_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0717_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0718_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0719_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0720_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0721_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0722_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0723_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0724_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0725_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0726_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0727_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0728_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0729_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0730_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0731_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0801_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0802_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0803_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0804_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0805_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0806_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0807_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0808_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0809_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0810_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0811_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0812_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0813_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0814_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0815_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0816_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0817_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0818_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB27.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR01.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR02.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR03.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR04.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR05.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR06.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR07.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR08.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR09.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR10.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR11.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR12.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR13.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR14.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR15.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR16.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR17.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR18.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR19.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR20.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR21.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR22.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR23.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR24.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR25.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR26.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR29.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR30.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR31.csv.gzip  \n"
          ]
        }
      ],
      "source": [
        "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXDcwYpvcX3"
      },
      "source": [
        "I extract the collection of tweets from the (23 of September ?? ) and I will work on this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Ir21JAqo7A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "\n",
        "data_list = []\n",
        "\n",
        "with gzip.open('1008_UkraineCombinedTweetsDeduped.csv.gzip', 'rt') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        data_list.append(row)\n",
        "\n",
        "rdd = sc.parallelize(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73CX7dxqvifD"
      },
      "outputs": [],
      "source": [
        "rdd.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU0jV4agws7o",
        "outputId": "bed30f89-c1ae-460b-ea5d-8c1e601a50f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('', '130404'),\n",
              "             ('userid', '1603709857'),\n",
              "             ('username', 'trajanpublisher'),\n",
              "             ('acctdesc',\n",
              "              \"We are Trajan Media, owner of Canada's premier numismatic and philatelic publications –\\xa0Canadian Coin News and Canadian Stamp News –\\xa0plus Coin & Stamp Supplies.\"),\n",
              "             ('location', 'Canada'),\n",
              "             ('following', '190'),\n",
              "             ('followers', '916'),\n",
              "             ('totaltweets', '7215'),\n",
              "             ('usercreatedts', '2013-07-18 15:23:57'),\n",
              "             ('tweetid', '1578535646146199554'),\n",
              "             ('tweetcreatedts', '2022-10-08 00:00:00'),\n",
              "             ('retweetcount', '1'),\n",
              "             ('text',\n",
              "              'Canada joined 15 countries whose flags grace the obverse of Ukraine’s first commemorative coin issued since the beginning of Russia’s renewed invasion this February.\\n\\nRead more: https://t.co/vRhROceH9x\\n\\n#numismatics #Ukraine️ #collectcoins'),\n",
              "             ('hashtags',\n",
              "              \"[{'text': 'numismatics', 'indices': [203, 215]}, {'text': 'Ukraine️', 'indices': [216, 225]}, {'text': 'collectcoins', 'indices': [226, 239]}]\"),\n",
              "             ('language', 'en'),\n",
              "             ('coordinates', ''),\n",
              "             ('favorite_count', '1'),\n",
              "             ('is_retweet', 'False'),\n",
              "             ('original_tweet_id', '0'),\n",
              "             ('original_tweet_userid', '0'),\n",
              "             ('original_tweet_username', ''),\n",
              "             ('in_reply_to_status_id', '0'),\n",
              "             ('in_reply_to_user_id', '0'),\n",
              "             ('in_reply_to_screen_name', ''),\n",
              "             ('is_quote_status', 'False'),\n",
              "             ('quoted_status_id', '0'),\n",
              "             ('quoted_status_userid', '0'),\n",
              "             ('quoted_status_username', ''),\n",
              "             ('extractedts', '2022-10-08 05:58:05.830392')])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZfZcFbiFVm"
      },
      "source": [
        "Tokenize tweet by removing puntuation and stop words in order to have only meaningful words to understand the meaning of the tweet. I also put all the words lower case and remove the link at the end of the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb1q3C3OxU6q",
        "outputId": "977ed491-9969-4774-8d0d-4c46002234a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 21.9 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=45edcd8c3d3e850ec6a4970c6088d4acbfdc3190effbc518f5221bce2d043b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKGpVdvFf1qs",
        "outputId": "2efedd46-e68d-492c-bdb9-c8d49663b27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from cleantext import clean\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopw = set(stopwords.words('english'))\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "# Without stopwords\n",
        "#tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation and word not in stopw]\n",
        "\n",
        "# With stopwords\n",
        "tokenize = lambda text: [word for word in tokenizer.tokenize(clean(text, no_urls=True, no_emails=True, no_emoji=True, no_line_breaks=True, replace_with_email=\"\", replace_with_url=\"\")) if word not in punctuation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjNQnsNLhLJo",
        "outputId": "7a9f489e-f03d-490c-e99f-bdfb48bd5ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " ['canada',\n",
              "  'joined',\n",
              "  '15',\n",
              "  'countries',\n",
              "  'whose',\n",
              "  'flags',\n",
              "  'grace',\n",
              "  'the',\n",
              "  'obverse',\n",
              "  'of',\n",
              "  \"ukraine's\",\n",
              "  'first',\n",
              "  'commemorative',\n",
              "  'coin',\n",
              "  'issued',\n",
              "  'since',\n",
              "  'the',\n",
              "  'beginning',\n",
              "  'of',\n",
              "  \"russia's\",\n",
              "  'renewed',\n",
              "  'invasion',\n",
              "  'this',\n",
              "  'february',\n",
              "  'read',\n",
              "  'more',\n",
              "  '#numismatics',\n",
              "  '#ukraine',\n",
              "  '#collectcoins'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sort(list):\n",
        "  list.sort()\n",
        "  return list\n",
        "\n",
        "en_tweets = rdd.filter(lambda x: x.get(\"language\")=='en').map(lambda x: (x, x.get(\"text\")))\n",
        "\n",
        "tweets_tokenized = en_tweets.map(lambda x: (x[0].get(\"tweetid\"), tokenize(x[1])))\n",
        "\n",
        "tweets_tokenized.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqYhIj4w2sM"
      },
      "source": [
        "## *K*-Shingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzQVwkvSNKwr",
        "outputId": "976effe9-b3ce-4964-8867-96a8392e8f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number K for k-grams dimension: 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1578535646146199554',\n",
              " ['#numismatics #ukraine #collectcoins',\n",
              "  '15 countries whose',\n",
              "  \"beginning of russia's\",\n",
              "  'canada joined 15',\n",
              "  'coin issued since',\n",
              "  'commemorative coin issued',\n",
              "  'countries whose flags',\n",
              "  'february read more',\n",
              "  'first commemorative coin',\n",
              "  'flags grace the',\n",
              "  'grace the obverse',\n",
              "  'invasion this february',\n",
              "  'issued since the',\n",
              "  'joined 15 countries',\n",
              "  'more #numismatics #ukraine',\n",
              "  \"obverse of ukraine's\",\n",
              "  \"of russia's renewed\",\n",
              "  \"of ukraine's first\",\n",
              "  'read more #numismatics',\n",
              "  'renewed invasion this',\n",
              "  \"russia's renewed invasion\",\n",
              "  'since the beginning',\n",
              "  'the beginning of',\n",
              "  'the obverse of',\n",
              "  'this february read',\n",
              "  \"ukraine's first commemorative\",\n",
              "  'whose flags grace'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "K = int(input(\"Enter the number K for k-grams dimension: \"))\n",
        "\n",
        "def build_shingles(tokens, K):\n",
        "  shingles = list()\n",
        "  tmp = \"\"\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    tmp = tokens[i]\n",
        "    if((len(tokens) - (i)) < K):\n",
        "      break;\n",
        "    for k in range(K - 1):\n",
        "      tmp += \" \" + tokens[i + (k + 1)]\n",
        "    shingles.append(tmp)\n",
        "\n",
        "  return shingles\n",
        "\n",
        "shingles_rdd = tweets_tokenized.map(lambda x: (x[0], sort(build_shingles(x[1], K))))\n",
        "shingles_rdd.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cast from list to set and then to list again in order to remove all the duplicate elements"
      ],
      "metadata": {
        "id": "hCHEA17OgvdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFcbUsQtcFA",
        "outputId": "18e246e8-eb74-4a1a-b8fa-60c3f5785d55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['##biggerthanme #osimhen #dazn',\n",
              " '##crimea #tma2022 #thefactmusicawards2022',\n",
              " '##crimea as #ukraine',\n",
              " '##dpr allied forces',\n",
              " '##droz #democraticviolence #gpexplorer',\n",
              " '##iranianlivesmatter #iranprotests2022 #tv3newday',\n",
              " '##kimjongun and #danielortega',\n",
              " '##kpworldtoursingapore #kerch #weverse',\n",
              " '##marvel #ironman #thor',\n",
              " '##nctdream #pakistanzindabad #case143']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "shingles_bag = sort(list(set(shingles_rdd.flatMap(lambda x: x[1]).collect())))\n",
        "shingles_bag[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WwWmOI1kJ9U"
      },
      "source": [
        "I use integers from 0 to *n* (with n equal to the length of shgles_bag) to store the shingles list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqt0SGYGd6eJ",
        "outputId": "cb4fac49-6941-4573-e197-bf6523298f7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572145"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "shingles_bag_id = list(range(0, len(shingles_bag)))\n",
        "len(shingles_bag_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convertShingles(shingles, BoS):\n",
        "  listToReturn = [0]*len(BoS)\n",
        "  start_index = 0;\n",
        "\n",
        "  for s in shingles:\n",
        "    for i in range(start_index, len(BoS)):\n",
        "      if(s == BoS[i]):\n",
        "        listToReturn[i] = 1\n",
        "        start_index = i\n",
        "        break;\n",
        "\n",
        "  return listToReturn\n",
        "\n",
        "shingles_int_rdd = shingles_rdd.map(lambda x: (x[0], convertShingles(x[1], shingles_bag)))\n",
        "#shingles_int_rdd.first()"
      ],
      "metadata": {
        "id": "kGqnNjt6wpid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hashing function for permutation"
      ],
      "metadata": {
        "id": "H-zwYzop2NDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generatePermutation(arrayToHash, a, b):\n",
        "  arrayToReturn = arrayToHash.copy()\n",
        "  \n",
        "  for i in range(len(arrayToHash)):\n",
        "    arrayToReturn[i] = (a * arrayToHash[i] + b) % len(arrayToHash)\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "n = int(input(\"Insert number n of permutation desired: \"))\n",
        "permutation_matrix = list()\n",
        "\n",
        "for i in range(n):\n",
        "  #Generate 2 random numbers in order to have different hashing by randomly chaging only these two numbers\n",
        "  a = np.random.randint(1000) \n",
        "  b = np.random.randint(1000)\n",
        "  #print(a, b)\n",
        "\n",
        "  permutation_matrix.append(generatePermutation(shingles_bag_id, a, b))\n",
        "  #print(permutation_matrix[i][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB5-jLT-2MJA",
        "outputId": "31fc74d4-6eb7-4901-949e-4663e95841a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insert number n of permutation desired: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateMinhashSignature(perm_mtx, shingle_array):\n",
        "  sig_column = [np.inf] * len(perm_mtx)\n",
        "\n",
        "  for i in range(len(shingle_array)):\n",
        "    if(shingle_array[i] == 1):\n",
        "      for j in range(len(sig_column)):\n",
        "        if(perm_mtx[j][i] < sig_column[j]):\n",
        "          sig_column[j] = perm_mtx[j][i]\n",
        "\n",
        "  return sig_column\n",
        "\n",
        "minhash_sig_rdd = shingles_int_rdd.map(lambda x: (x[0], calculateMinhashSignature(permutation_matrix, x[1])))\n",
        "minhash_sig_rdd.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHz1RlZcGVYz",
        "outputId": "405f4b7a-9e30-4c29-88e6-397055d3dbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1578535646146199554',\n",
              "  [9400, 6009, 32616, 434, 104005, 3699, 21690, 28338, 10035, 18091]),\n",
              " ('1578535651984650240',\n",
              "  [50652, 11523, 71706, 1899, 97051, 6659, 20245, 12128, 35965, 42463]),\n",
              " ('1578535652248850432',\n",
              "  [160179, 2847, 43961, 26264, 83581, 15714, 14311, 23464, 49945, 1114]),\n",
              " ('1578535654149038080',\n",
              "  [12027, 65733, 26391, 108454, 14695, 3748, 243068, 17376, 15995, 86638]),\n",
              " ('1578535658691629056',\n",
              "  [61506, 3522, 14936, 5684, 5893, 34505, 16264, 11078, 1440, 11194]),\n",
              " ('1578535679713497089',\n",
              "  [2493, 15150, 27076, 5239, 2743, 2595, 13129, 2563, 14175, 15628]),\n",
              " ('1578535680585916417',\n",
              "  [3613, 18600, 9671, 38239, 12229, 2675, 7576, 10103, 10505, 19840]),\n",
              " ('1578535686818304000',\n",
              "  [1088, 876, 26146, 10769, 1147, 3775, 1283, 3879, 12805, 37270]),\n",
              " ('1578535704694779907',\n",
              "  [103050, 1830, 94061, 507499, 323395, 160810, 16770, 147275, 83415, 349705]),\n",
              " ('1578535714945658882',\n",
              "  [175, 28038, 17256, 299, 7309, 22374, 50577, 725, 11825, 64930])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = int(input(\"Enter into how many bands you want to divide the \" + str(n) +\" permutations: \"))\n",
        "\n",
        "def band_splitting(tweet_id, arrayToSplit, b, n):\n",
        "  tmp = list()\n",
        "  arrayToReturn = list()\n",
        "  band_id = 1\n",
        "\n",
        "  r = int(n/b)\n",
        "  i = 0\n",
        "\n",
        "  while i < len(arrayToSplit):\n",
        "    for j in range(r):\n",
        "      tmp.append(arrayToSplit[i + j])\n",
        "    \n",
        "    i += r\n",
        "    arrayToReturn.append((tweet_id, (band_id, tmp.copy())))\n",
        "    band_id += 1\n",
        "    tmp.clear()\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "sig_bands_rdd = minhash_sig_rdd.flatMap(lambda x: band_splitting(x[0], x[1], b, n))\n",
        "sig_bands_rdd.take(10)"
      ],
      "metadata": {
        "id": "NlFPde9vRF12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def hashArray(array):\n",
        "  col_str = ' '.join(str(x) for x in array)\n",
        "\n",
        "  return hash(col_str)\n",
        "  #return int(hashlib.sha1(col_str.encode('utf-8')).hexdigest(), 16)\n",
        "\n",
        "sig_bands_hashed_rdd = sig_bands_rdd.map(lambda x: ((hashArray(x[1][1]), x[1][0]), x[0]))\n",
        "sig_bands_hashed_rdd.persist()\n",
        "#sig_bands_hashed_rdd.count()"
      ],
      "metadata": {
        "id": "ouUR4pGXak6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68596c48-e505-4209-871e-11e10985b9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[27] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sig_bands_hashed_rdd.take(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "SXQlMFsP7SA6",
        "outputId": "dfa49643-45b6-483c-b03b-d42636570cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-89f45c64f043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msig_bands_hashed_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32mspark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEBUG"
      ],
      "metadata": {
        "id": "xZONxbU1SCHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generatePermutation(arrayToHash, a, b):\n",
        "  arrayToReturn = arrayToHash.copy()\n",
        "  \n",
        "  for i in range(len(arrayToHash)):\n",
        "    arrayToReturn[i] = (a * arrayToHash[i] + b) % len(arrayToHash)\n",
        "\n",
        "  #print(arrayToHash, a, b, arrayToReturn)\n",
        "\n",
        "  return arrayToReturn\n",
        "  \n",
        "def calculateMinhashSignature(perm_mtx, shingle_array):\n",
        "  sig_column = [np.inf] * len(perm_mtx)\n",
        "\n",
        "  for i in range(len(shingle_array)):\n",
        "    if(shingle_array[i] == 1):\n",
        "      for j in range(len(sig_column)):\n",
        "        if(perm_mtx[j][i] < sig_column[j]):\n",
        "          #print(perm_mtx[j][i])\n",
        "          sig_column[j] = perm_mtx[j][i]\n",
        "\n",
        "  return sig_column\n",
        "\n",
        "n = int(input(\"Insert number n of permutation desired: \"))\n",
        "permutation_matrix = list()\n",
        "tmp = list(range(0, 10))\n",
        "array = [0,0,0,0,1,0,0,1,1,0]\n",
        "\n",
        "for i in range(n):\n",
        "  #Generate 2 random numbers in order to have different hashing by randomly chaging only these two numbers\n",
        "  a = np.random.randint(10) \n",
        "  b = np.random.randint(10)\n",
        "\n",
        "  permutation_matrix.append(generatePermutation(tmp, a, b))\n",
        "  \n",
        "#[print(*line) for line in permutation_matrix]\n",
        "  \n",
        "a = calculateMinhashSignature(permutation_matrix, array)\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia4-iv1RPZTn",
        "outputId": "eed865e2-60eb-496a-a297-a19d15541f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert number n of permutation desired: 10\n",
            "[3, 1, 1, 6, 2, 4, 1, 1, 0, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def band_splitting(tweet_id, arrayToSplit, b, n):\n",
        "  tmp = list()\n",
        "  arrayToReturn = list()\n",
        "  band_id = 1\n",
        "\n",
        "  r = int(n/b)\n",
        "  i = 0\n",
        "  while i < len(arrayToSplit):\n",
        "    print(i)\n",
        "    for j in range(r):\n",
        "      tmp.append(arrayToSplit[i + j])\n",
        "    i += r\n",
        "    print(tmp)\n",
        "    arrayToReturn.append((tweet_id, (band_id, tmp.copy())))\n",
        "    band_id += 1\n",
        "    tmp.clear()\n",
        "\n",
        "  return arrayToReturn\n",
        "\n",
        "b = 5\n",
        "ex = band_splitting(\"xx\", a, b, 10)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eqXB6ckWBGU",
        "outputId": "03ae8ec9-cb39-4f30-d5b8-a59eed5ecc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[3, 1]\n",
            "2\n",
            "[1, 6]\n",
            "4\n",
            "[2, 4]\n",
            "6\n",
            "[1, 1]\n",
            "8\n",
            "[0, 4]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('xx', (1, [3, 1])),\n",
              " ('xx', (2, [1, 6])),\n",
              " ('xx', (3, [2, 4])),\n",
              " ('xx', (4, [1, 1])),\n",
              " ('xx', (5, [0, 4]))]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "a = \"cnoefvrivpvòslkfdvmkfvpòfvevflàeràòwwwwedweferfeev\"\n",
        "\n",
        "#b = int(hashlib.sha1(str(a).encode('utf-8')).hexdigest(), 16)\n",
        "b = hashlib.sha1(str(a).encode('utf-8')).hexdigest()\n",
        "#int(hashlib.sha1(str(a).encode('utf-8') + self.salt.encode('utf-8')).hexdigest()[-self.resultSize:], 16)\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "R-G5c_qppOvc",
        "outputId": "470d226e-39b3-4df7-ad8d-1248cafdbe30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'75c93d49c4b7c23b5ba3abde96216cfe2355bd82'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#a = [1,2,3,4,5,6,7,8]\n",
        "a = ['a','b','c','d']\n",
        "\n",
        "b = ' '.join(str(x) for x in a)\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DGuB4iQwu7Ac",
        "outputId": "177109e9-85a7-4a25-a0e3-4d024496909c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a b c d'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TO DO:\n",
        "- Hashing list on shingles\n",
        "- Verify if the hash is worth in terms of space "
      ],
      "metadata": {
        "id": "dxwfbUXOvN7i"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOd5bExm7o5M7yY4DLK/LA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}